COMPREHENSIVE AUDIT: Platform Integration & Feature Completeness

Before Phase 2 proceeds, assess the current state holistically. DO NOT make changes.

=== PART 1: USE CASE AS FOUNDATION ===

1. What fields on use_cases are USER-ENTERED vs SYSTEM-DERIVED vs SYSTEM-CALCULATED?
   
   List each field and categorize:
   - User-entered: title, description, scores, etc.
   - System-derived: TOM phase (from status/deployment), etc.
   - System-calculated: T-shirt cost estimates, etc.

2. When a use case is fetched via API (GET /api/use-cases/:id), which derived/calculated values are included in the response?
   □ TOM phase (derived)
   □ T-shirt sizing costs (calculated)
   □ Impact/effort scores (calculated from 10 levers)
   □ Quadrant assignment (derived from scores)
   □ Value estimates (if implemented)

3. Are derived values stored on the use case record, or calculated on-the-fly?
   - If stored: when are they updated?
   - If on-the-fly: is logic consistent across all API endpoints?

=== PART 2: CRUD MODAL COMPLETENESS ===

4. In the main use case CRUD modal (CRUDUseCaseModal), which derived/calculated values are VISIBLE to the user?

   □ TOM Phase badge/indicator
   □ T-shirt sizing summary
   □ Impact/Effort scores
   □ Quadrant assignment
   □ Value estimates (if applicable)
   
   For each: Is it read-only display or editable?

5. Are there values that appear in Dashboard/Explorer but NOT in the CRUD modal?

=== PART 3: FEATURE COMPLETENESS MATRIX ===

6. For each major feature, assess completeness:

   TOM (Target Operating Model):
   □ Schema: metadata_config.tomConfig
   □ Schema: use_cases TOM fields
   □ API: Config endpoints
   □ API: Phase derivation in use case response
   □ CRUD Modal: Phase visible
   □ Explorer: Phase filter/column
   □ Dashboard: Phase breakdown
   □ Admin: TOM Configuration tab
   □ Documentation: replit.md updated

   T-SHIRT SIZING:
   □ Schema: metadata_config.tShirtSizing
   □ Schema: use_cases sizing fields
   □ API: Sizing calculation
   □ CRUD Modal: Sizing visible/editable
   □ Explorer: Sizing column
   □ Dashboard: Resource planning
   □ Admin: Sizing configuration
   □ Documentation: replit.md

   SCORING (10-lever):
   □ Schema: score fields on use_cases
   □ Schema: metadata_config.scoringDropdownOptions
   □ API: Score calculation (impact/effort)
   □ CRUD Modal: Scoring tab
   □ Explorer: Score columns
   □ Dashboard: Score-based analytics
   □ Admin: Scoring options config
   □ Documentation

=== PART 4: ADMIN MODULE COVERAGE ===

7. List ALL configurable aspects of the platform and whether Admin UI exists:

   | Configuration | Admin UI Exists? | Location |
   |---------------|------------------|----------|
   | Metadata (processes, LOBs, etc.) | ? | ? |
   | Scoring dropdown options | ? | ? |
   | T-shirt sizing rates | ? | ? |
   | TOM phases/presets | ? | ? |
   | Value realization KPIs | ? | ? |
   | Other? | ? | ? |

=== PART 5: AGGREGATION CONSISTENCY ===

8. For portfolio-level aggregations, where does the calculation happen?

   | Aggregation | Calculated Where | Consistent with Use Case Data? |
   |-------------|------------------|-------------------------------|
   | Total use cases per quadrant | ? | ? |
   | Use cases per TOM phase | ? | ? |
   | Portfolio investment | ? | ? |
   | Resource planning totals | ? | ? |

=== PART 6: GAPS & RECOMMENDATIONS ===

9. Based on above, identify:
   - Fields that should be derived but aren't
   - Views that are missing derived values
   - Admin configurations that are missing
   - Aggregations that might be inconsistent

10. Recommend any architectural changes needed BEFORE adding Value Realization

=== OUTPUT FORMAT ===

## PLATFORM INTEGRATION AUDIT

### Use Case Field Classification
[Table of fields: user-entered / derived / calculated]

### CRUD Modal Visibility
[What's shown vs what's missing]

### Feature Completeness Matrix
[Per-feature checklist results]

### Admin Coverage
[What's configurable vs what's missing]

### Aggregation Consistency
[Where calculations happen, any inconsistencies]

### Critical Gaps
[Prioritized list of issues]

### Recommendations Before Phase 2
[What should be fixed first]